{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AaFJ5bUTwhiV",
        "outputId": "d7a1ce9a-69d6-48f4-8c32-432da3808444"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspellchecker in /usr/local/lib/python3.11/dist-packages (0.8.2)\n",
            "Requirement already satisfied: PyPDF2 in /usr/local/lib/python3.11/dist-packages (3.0.1)\n",
            "Requirement already satisfied: fpdf in /usr/local/lib/python3.11/dist-packages (1.7.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspellchecker\n",
        "!pip install PyPDF2\n",
        "!pip install fpdf\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from PyPDF2 import PdfReader  # extract text from PDFs\n",
        "import re  # handle text processing with regular expressions\n",
        "import pandas as pd  # data manipulation and storage\n",
        "from collections import Counter  # count word occurrences\n",
        "from spellchecker import SpellChecker  # detect misspelled or non-English words\n",
        "from fpdf import FPDF  # create PDF reports\n",
        "import matplotlib.pyplot as plt  # generate visualizations\n"
      ],
      "metadata": {
        "id": "5aBfhFzSwsWI"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PyPDF2 import PdfReader\n",
        "import re\n",
        "import random\n",
        "\n",
        "PDF_FILE = \"/content/Harry_Potter_(www.ztcprep.com).pdf\"\n",
        "OUTPUT_FILE1 = \"file1.txt\"\n",
        "OUTPUT_FILE2 = \"file2.txt\"\n",
        "\n",
        "BIRTH_MONTH, BIRTH_DATE, BIRTH_YEAR = 7, 23, 2002\n",
        "BOOK_ID = 7\n",
        "START_PAGE1 = BIRTH_DATE+1  # pages 24-33\n",
        "START_PAGE2 = 102 +1  # pages 103-112\n",
        "\n",
        "def extract_pages(pdf_path, start_page, num_pages=10):\n",
        "    \"\"\"Extracts text from a specified page range in a PDF file.\"\"\"\n",
        "    reader = PdfReader(pdf_path)\n",
        "    total_pages = len(reader.pages)\n",
        "\n",
        "    start_index = max(0, start_page - 1)  # Convert to zero-based index\n",
        "    end_index = min(start_index + num_pages, total_pages)  # Ensure within bounds\n",
        "\n",
        "    extracted_text = []\n",
        "    for i in range(start_index, end_index):\n",
        "        text = reader.pages[i].extract_text()\n",
        "        if text:\n",
        "            extracted_text.append(text)\n",
        "\n",
        "    return \"\\n\".join(extracted_text)\n",
        "\n",
        "def paraphrase_text(text):\n",
        "    \"\"\"Simple paraphrasing by replacing words with synonyms and restructuring sentences.\"\"\"\n",
        "    if not text.strip():\n",
        "        return \"No valid text extracted.\"\n",
        "\n",
        "    # Synonyms mapping (Basic replacement method)\n",
        "    synonyms = {\n",
        "        \"said\": \"stated\", \"looked\": \"glanced\", \"ran\": \"sprinted\", \"saw\": \"noticed\",\n",
        "        \"house\": \"home\", \"boy\": \"young man\", \"girl\": \"young woman\", \"wizard\": \"sorcerer\"\n",
        "    }\n",
        "\n",
        "    words = text.split()\n",
        "    paraphrased_words = [synonyms.get(word, word) for word in words]\n",
        "\n",
        "    paraphrased_text = \" \".join(paraphrased_words)\n",
        "\n",
        "    # Random sentence restructuring (Basic shuffle)\n",
        "    sentences = re.split(r'(\\.|\\?|!)', paraphrased_text)  # Split into sentences\n",
        "    sentences = [\"\".join(x) for x in zip(sentences[0::2], sentences[1::2])]  # Merge punctuation back\n",
        "    random.shuffle(sentences)  # Shuffle to change order\n",
        "\n",
        "    return \" \".join(sentences)\n",
        "\n",
        "try:\n",
        "    extracted_text1 = extract_pages(PDF_FILE, START_PAGE1)\n",
        "    extracted_text2 = extract_pages(PDF_FILE, START_PAGE2)\n",
        "\n",
        "    # Apply paraphrasing\n",
        "    paraphrased_text1 = paraphrase_text(extracted_text1)\n",
        "    paraphrased_text2 = paraphrase_text(extracted_text2)\n",
        "\n",
        "    with open(OUTPUT_FILE1, \"w\", encoding=\"utf-8\") as f1, open(OUTPUT_FILE2, \"w\", encoding=\"utf-8\") as f2:\n",
        "        f1.write(paraphrased_text1)\n",
        "        f2.write(paraphrased_text2)\n",
        "\n",
        "    print(f\"Text extraction and paraphrasing complete: {OUTPUT_FILE1}, {OUTPUT_FILE2}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error: {e}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hzzIO0zCwy-7",
        "outputId": "13df0b00-9d71-4fad-a88f-b9babf6c083f"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text extraction and paraphrasing complete: file1.txt, file2.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q1**:  Write Python code and use MapReduct to count occurrences of each word in the first text file (file.txt). How many times each word is repeated?"
      ],
      "metadata": {
        "id": "FvaYsCShw5ro"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import csv\n",
        "from collections import Counter\n",
        "\n",
        "TEXT_FILE = \"/content/file1.txt\"\n",
        "OUTPUT_CSV = \"word_count.txt\"\n",
        "\n",
        "def extract_words(text):\n",
        "    \"\"\"Extracts words from text and converts them to lowercase.\"\"\"\n",
        "    return re.findall(r'\\b\\w+\\b', text.lower())\n",
        "\n",
        "# Read text from file\n",
        "try:\n",
        "    with open(TEXT_FILE, \"r\", encoding=\"utf-8\") as file:\n",
        "        content = file.read()\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: The file {TEXT_FILE} was not found.\")\n",
        "    exit()\n",
        "\n",
        "# Count word occurrences\n",
        "word_freq = Counter(extract_words(content))\n",
        "\n",
        "# Save results to CSV\n",
        "with open(OUTPUT_CSV, \"w\", newline=\"\", encoding=\"utf-8\") as csv_file:\n",
        "    writer = csv.writer(csv_file)\n",
        "    writer.writerow([\"Word\", \"Count\"])\n",
        "    writer.writerows(word_freq.most_common())  # Sorts by frequency automatically\n",
        "\n",
        "# Display output in a structured format\n",
        "print(\"\\nWord Frequency Analysis from file1.txt:\")\n",
        "for word, count in word_freq.most_common():\n",
        "    print(f\"{word}: {count}\")\n"
      ],
      "metadata": {
        "id": "pWzLVb7Qw8SP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0119d35-7d23-4ee6-b78a-44f29aae16cb"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Word Frequency Analysis from file1.txt:\n",
            "the: 81\n",
            "and: 49\n",
            "he: 44\n",
            "a: 40\n",
            "it: 30\n",
            "of: 28\n",
            "to: 26\n",
            "s: 25\n",
            "dumbledore: 23\n",
            "i: 23\n",
            "harry: 23\n",
            "his: 23\n",
            "t: 19\n",
            "on: 18\n",
            "professor: 18\n",
            "that: 17\n",
            "as: 17\n",
            "in: 17\n",
            "mcgonagall: 16\n",
            "you: 15\n",
            "was: 15\n",
            "hagrid: 14\n",
            "stated: 13\n",
            "all: 13\n",
            "e: 12\n",
            "him: 12\n",
            "potter: 12\n",
            "out: 11\n",
            "be: 11\n",
            "ztcprep: 10\n",
            "www: 10\n",
            "can: 10\n",
            "but: 10\n",
            "y: 9\n",
            "at: 9\n",
            "over: 9\n",
            "her: 9\n",
            "com: 9\n",
            "up: 8\n",
            "had: 8\n",
            "g: 8\n",
            "w: 7\n",
            "k: 7\n",
            "rowling: 7\n",
            "very: 7\n",
            "p: 7\n",
            "philosophers: 7\n",
            "stone: 7\n",
            "j: 7\n",
            "couldn: 7\n",
            "they: 7\n",
            "could: 6\n",
            "front: 6\n",
            "with: 6\n",
            "not: 6\n",
            "here: 6\n",
            "ve: 6\n",
            "man: 6\n",
            "this: 6\n",
            "re: 6\n",
            "blankets: 5\n",
            "know: 5\n",
            "back: 5\n",
            "street: 5\n",
            "ll: 5\n",
            "have: 5\n",
            "their: 5\n",
            "letter: 5\n",
            "little: 5\n",
            "f: 5\n",
            "people: 5\n",
            "for: 5\n",
            "them: 5\n",
            "sir: 5\n",
            "motorcycle: 5\n",
            "would: 5\n",
            "how: 5\n",
            "no: 5\n",
            "see: 4\n",
            "bundle: 4\n",
            "took: 4\n",
            "es: 4\n",
            "into: 4\n",
            "get: 4\n",
            "will: 4\n",
            "who: 4\n",
            "glanced: 4\n",
            "famous: 4\n",
            "like: 4\n",
            "were: 4\n",
            "around: 4\n",
            "she: 4\n",
            "saying: 4\n",
            "kill: 4\n",
            "young: 4\n",
            "just: 3\n",
            "number: 3\n",
            "four: 3\n",
            "put: 3\n",
            "turned: 3\n",
            "down: 3\n",
            "privet: 3\n",
            "drive: 3\n",
            "suddenly: 3\n",
            "muggles: 3\n",
            "is: 3\n",
            "dursleys: 3\n",
            "door: 3\n",
            "almost: 3\n",
            "been: 3\n",
            "night: 3\n",
            "when: 3\n",
            "live: 3\n",
            "we: 3\n",
            "ou: 3\n",
            "d: 3\n",
            "by: 3\n",
            "something: 3\n",
            "one: 3\n",
            "under: 3\n",
            "if: 3\n",
            "from: 3\n",
            "about: 3\n",
            "come: 3\n",
            "place: 3\n",
            "black: 3\n",
            "only: 3\n",
            "eyes: 3\n",
            "gone: 3\n",
            "cloak: 3\n",
            "inside: 3\n",
            "then: 3\n",
            "why: 3\n",
            "got: 3\n",
            "right: 3\n",
            "knowing: 3\n",
            "step: 2\n",
            "may: 2\n",
            "never: 2\n",
            "corner: 2\n",
            "walked: 2\n",
            "sitting: 2\n",
            "wall: 2\n",
            "twelve: 2\n",
            "light: 2\n",
            "so: 2\n",
            "where: 2\n",
            "rose: 2\n",
            "same: 2\n",
            "tidy: 2\n",
            "which: 2\n",
            "exactly: 2\n",
            "feet: 2\n",
            "c: 2\n",
            "an: 2\n",
            "yes: 2\n",
            "or: 2\n",
            "whispered: 2\n",
            "low: 2\n",
            "what: 2\n",
            "don: 2\n",
            "mean: 2\n",
            "suppose: 2\n",
            "way: 2\n",
            "written: 2\n",
            "handkerchief: 2\n",
            "face: 2\n",
            "even: 2\n",
            "fell: 2\n",
            "asleep: 2\n",
            "my: 2\n",
            "left: 2\n",
            "huge: 2\n",
            "wouldn: 2\n",
            "gave: 2\n",
            "great: 2\n",
            "watch: 2\n",
            "pocket: 2\n",
            "ell: 2\n",
            "better: 2\n",
            "v: 2\n",
            "woken: 2\n",
            "find: 2\n",
            "changed: 2\n",
            "dursley: 2\n",
            "two: 2\n",
            "best: 2\n",
            "expect: 2\n",
            "things: 2\n",
            "hair: 2\n",
            "hands: 2\n",
            "baby: 2\n",
            "away: 2\n",
            "think: 2\n",
            "trust: 2\n",
            "life: 2\n",
            "roar: 2\n",
            "air: 2\n",
            "though: 2\n",
            "day: 2\n",
            "did: 2\n",
            "other: 2\n",
            "arms: 2\n",
            "aunt: 2\n",
            "uncle: 2\n",
            "holding: 2\n",
            "really: 2\n",
            "last: 2\n",
            "me: 2\n",
            "explain: 2\n",
            "there: 2\n",
            "name: 2\n",
            "glasses: 2\n",
            "before: 2\n",
            "opened: 2\n",
            "said: 2\n",
            "few: 2\n",
            "good: 2\n",
            "boy: 2\n",
            "fled: 2\n",
            "sky: 2\n",
            "bent: 2\n",
            "head: 2\n",
            "voice: 2\n",
            "must: 2\n",
            "son: 2\n",
            "stopped: 1\n",
            "silver: 1\n",
            "outer: 1\n",
            "repeated: 1\n",
            "faintly: 1\n",
            "scar: 1\n",
            "forever: 1\n",
            "clicked: 1\n",
            "once: 1\n",
            "balls: 1\n",
            "sped: 1\n",
            "lamps: 1\n",
            "glowed: 1\n",
            "orange: 1\n",
            "make: 1\n",
            "hissed: 1\n",
            "wake: 1\n",
            "sun: 1\n",
            "gardens: 1\n",
            "lit: 1\n",
            "brass: 1\n",
            "crept: 1\n",
            "living: 1\n",
            "room: 1\n",
            "mr: 1\n",
            "pointing: 1\n",
            "shhh: 1\n",
            "stand: 1\n",
            "lily: 1\n",
            "james: 1\n",
            "dead: 1\n",
            "poor: 1\n",
            "ter: 1\n",
            "sad: 1\n",
            "grip: 1\n",
            "yourself: 1\n",
            "found: 1\n",
            "patting: 1\n",
            "gingerly: 1\n",
            "arm: 1\n",
            "stepped: 1\n",
            "garden: 1\n",
            "these: 1\n",
            "understand: 1\n",
            "nodded: 1\n",
            "glumly: 1\n",
            "does: 1\n",
            "tend: 1\n",
            "13: 1\n",
            "full: 1\n",
            "minute: 1\n",
            "three: 1\n",
            "stood: 1\n",
            "shoulders: 1\n",
            "shook: 1\n",
            "blinked: 1\n",
            "furiously: 1\n",
            "told: 1\n",
            "sorry: 1\n",
            "sobbed: 1\n",
            "taking: 1\n",
            "lar: 1\n",
            "ge: 1\n",
            "spotted: 1\n",
            "burying: 1\n",
            "won: 1\n",
            "remember: 1\n",
            "flyin: 1\n",
            "bristol: 1\n",
            "myself: 1\n",
            "above: 1\n",
            "knee: 1\n",
            "perfect: 1\n",
            "map: 1\n",
            "london: 1\n",
            "ground: 1\n",
            "borrowed: 1\n",
            "giant: 1\n",
            "climbing: 1\n",
            "carefully: 1\n",
            "spoke: 1\n",
            "nothing: 1\n",
            "astride: 1\n",
            "snif: 1\n",
            "golden: 1\n",
            "examined: 1\n",
            "give: 1\n",
            "anashig: 1\n",
            "glass: 1\n",
            "nearly: 1\n",
            "ten: 1\n",
            "years: 1\n",
            "passed: 1\n",
            "since: 1\n",
            "nephew: 1\n",
            "hardly: 1\n",
            "seen: 1\n",
            "fateful: 1\n",
            "news: 1\n",
            "report: 1\n",
            "owls: 1\n",
            "19: 1\n",
            "believe: 1\n",
            "oh: 1\n",
            "albus: 1\n",
            "reached: 1\n",
            "patted: 1\n",
            "shoulder: 1\n",
            "are: 1\n",
            "less: 1\n",
            "us: 1\n",
            "blew: 1\n",
            "nose: 1\n",
            "reply: 1\n",
            "scars: 1\n",
            "handy: 1\n",
            "firmly: 1\n",
            "astonishing: 1\n",
            "happen: 1\n",
            "simply: 1\n",
            "too: 1\n",
            "big: 1\n",
            "allowed: 1\n",
            "wild: 1\n",
            "long: 1\n",
            "tangles: 1\n",
            "bushy: 1\n",
            "beard: 1\n",
            "hid: 1\n",
            "most: 1\n",
            "size: 1\n",
            "trash: 1\n",
            "lids: 1\n",
            "leather: 1\n",
            "boots: 1\n",
            "dolphins: 1\n",
            "grew: 1\n",
            "steadily: 1\n",
            "louder: 1\n",
            "some: 1\n",
            "sign: 1\n",
            "headlight: 1\n",
            "rumbling: 1\n",
            "sound: 1\n",
            "broken: 1\n",
            "silence: 1\n",
            "growing: 1\n",
            "until: 1\n",
            "ready: 1\n",
            "take: 1\n",
            "guess: 1\n",
            "twinkling: 1\n",
            "usually: 1\n",
            "shone: 1\n",
            "seemed: 1\n",
            "cried: 1\n",
            "jumping: 1\n",
            "14: 1\n",
            "wise: 1\n",
            "important: 1\n",
            "iping: 1\n",
            "streaming: 1\n",
            "jacket: 1\n",
            "sleeve: 1\n",
            "swung: 1\n",
            "himself: 1\n",
            "onto: 1\n",
            "kicked: 1\n",
            "engine: 1\n",
            "eyed: 1\n",
            "thought: 1\n",
            "might: 1\n",
            "hiding: 1\n",
            "underneath: 1\n",
            "rolled: 1\n",
            "without: 1\n",
            "waking: 1\n",
            "watching: 1\n",
            "twice: 1\n",
            "tall: 1\n",
            "normal: 1\n",
            "least: 1\n",
            "five: 1\n",
            "times: 1\n",
            "wide: 1\n",
            "shall: 1\n",
            "soon: 1\n",
            "nodding: 1\n",
            "doorstep: 1\n",
            "tucked: 1\n",
            "came: 1\n",
            "toward: 1\n",
            "house: 1\n",
            "bring: 1\n",
            "bringing: 1\n",
            "odd: 1\n",
            "vast: 1\n",
            "muscular: 1\n",
            "photographs: 1\n",
            "mantelpiece: 1\n",
            "showed: 1\n",
            "going: 1\n",
            "tell: 1\n",
            "places: 1\n",
            "home: 1\n",
            "destroyed: 1\n",
            "able: 1\n",
            "everything: 1\n",
            "older: 1\n",
            "faltered: 1\n",
            "legend: 1\n",
            "surprised: 1\n",
            "today: 1\n",
            "known: 1\n",
            "future: 1\n",
            "books: 1\n",
            "every: 1\n",
            "child: 1\n",
            "our: 1\n",
            "world: 1\n",
            "knows: 1\n",
            "oldemort: 1\n",
            "power: 1\n",
            "somehow: 1\n",
            "broke: 1\n",
            "looking: 1\n",
            "seriously: 1\n",
            "top: 1\n",
            "half: 1\n",
            "moon: 1\n",
            "started: 1\n",
            "swarmin: 1\n",
            "mouth: 1\n",
            "mind: 1\n",
            "swallowed: 1\n",
            "course: 1\n",
            "small: 1\n",
            "hand: 1\n",
            "closed: 1\n",
            "beside: 1\n",
            "slept: 1\n",
            "special: 1\n",
            "hours: 1\n",
            "time: 1\n",
            "mrs: 1\n",
            "say: 1\n",
            "bye: 1\n",
            "scream: 1\n",
            "milk: 1\n",
            "bottles: 1\n",
            "nor: 1\n",
            "spend: 1\n",
            "next: 1\n",
            "weeks: 1\n",
            "being: 1\n",
            "prodded: 1\n",
            "pinched: 1\n",
            "cousin: 1\n",
            "dudley: 1\n",
            "do: 1\n",
            "finally: 1\n",
            "much: 1\n",
            "moment: 1\n",
            "meeting: 1\n",
            "secret: 1\n",
            "country: 1\n",
            "numbers: 1\n",
            "instead: 1\n",
            "planets: 1\n",
            "moving: 1\n",
            "edge: 1\n",
            "true: 1\n",
            "visible: 1\n",
            "fast: 1\n",
            "astounding: 1\n",
            "stop: 1\n",
            "heaven: 1\n",
            "survive: 1\n",
            "sounding: 1\n",
            "relieved: 1\n",
            "after: 1\n",
            "done: 1\n",
            "killed: 1\n",
            "breeze: 1\n",
            "ruf: 1\n",
            "neat: 1\n",
            "hedges: 1\n",
            "lay: 1\n",
            "silent: 1\n",
            "inky: 1\n",
            "18: 1\n",
            "business: 1\n",
            "staying: 1\n",
            "well: 1\n",
            "go: 1\n",
            "join: 1\n",
            "celebrations: 1\n",
            "16: 1\n",
            "forward: 1\n",
            "tuft: 1\n",
            "jet: 1\n",
            "forehead: 1\n",
            "curiously: 1\n",
            "shaped: 1\n",
            "cut: 1\n",
            "bolt: 1\n",
            "lightning: 1\n",
            "15: 1\n",
            "tabby: 1\n",
            "cat: 1\n",
            "slinking: 1\n",
            "end: 1\n",
            "luck: 1\n",
            "murmured: 1\n",
            "shaggy: 1\n",
            "problems: 1\n",
            "swelled: 1\n",
            "both: 1\n",
            "landed: 1\n",
            "road: 1\n",
            "eah: 1\n",
            "muf: 1\n",
            "bike: 1\n",
            "heel: 1\n",
            "swish: 1\n",
            "made: 1\n",
            "sense: 1\n",
            "because: 1\n",
            "late: 1\n",
            "laid: 1\n",
            "gently: 1\n",
            "17: 1\n",
            "m: 1\n",
            "heart: 1\n",
            "isn: 1\n",
            "grudgingly: 1\n",
            "pretend: 1\n",
            "careless: 1\n",
            "enough: 1\n",
            "turn: 1\n",
            "any: 1\n",
            "scratchy: 1\n",
            "whiskery: 1\n",
            "kiss: 1\n",
            "oung: 1\n",
            "sirius: 1\n",
            "lent: 1\n",
            "pulled: 1\n",
            "lace: 1\n",
            "dabbed: 1\n",
            "beneath: 1\n",
            "spectacles: 1\n",
            "tried: 1\n",
            "potters: 1\n",
            "asked: 1\n",
            "trembled: 1\n",
            "went: 1\n",
            "heavily: 1\n",
            "getting: 1\n",
            "family: 1\n",
            "has: 1\n",
            "now: 1\n",
            "walk: 1\n",
            "talk: 1\n",
            "hushed: 1\n",
            "voices: 1\n",
            "o: 1\n",
            "lived: 1\n",
            "let: 1\n",
            "howl: 1\n",
            "wounded: 1\n",
            "dog: 1\n",
            "noticed: 1\n",
            "kicking: 1\n",
            "mother: 1\n",
            "screaming: 1\n",
            "sweets: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q2**: From the second text file (file2.txt), write Python code and use MapReduct to count how many times non-English words (names, places, spells etc.) were used. List those words and how many times each was repeated.\n",
        "\n"
      ],
      "metadata": {
        "id": "00Vzhnarw-iw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import csv\n",
        "from collections import Counter\n",
        "from spellchecker import SpellChecker\n",
        "\n",
        "FILE_PATH = \"/content/file2.txt\"\n",
        "OUTPUT_FILE = \"non-english-words.txt\"\n",
        "\n",
        "# Initialize spell checker\n",
        "spell_checker = SpellChecker()\n",
        "\n",
        "def get_words(text):\n",
        "    \"\"\"Extract words from text and convert them to lowercase.\"\"\"\n",
        "    return re.findall(r'\\b\\w+\\b', text.lower())\n",
        "\n",
        "# Read text from file\n",
        "try:\n",
        "    with open(FILE_PATH, \"r\", encoding=\"utf-8\") as file:\n",
        "        content = file.read()\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: The file {FILE_PATH} was not found.\")\n",
        "    exit()\n",
        "\n",
        "# Extract words and filter non-English words\n",
        "words = get_words(content)\n",
        "non_english = [word for word in words if word not in spell_checker]\n",
        "\n",
        "# Count occurrences\n",
        "word_counts = Counter(non_english)\n",
        "\n",
        "# Save results to CSV\n",
        "with open(OUTPUT_FILE, \"w\", newline=\"\", encoding=\"utf-8\") as csv_file:\n",
        "    writer = csv.writer(csv_file)\n",
        "    writer.writerow([\"Non-English Word\", \"Count\"])\n",
        "    writer.writerows(word_counts.most_common())  # Sorts by frequency automatically\n",
        "\n",
        "# Display output\n",
        "print(\"\\nIdentified Non-English Words from file2.txt:\")\n",
        "for word, count in word_counts.most_common():\n",
        "    print(f\"{word}: {count}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oDjnsANqxEFc",
        "outputId": "ad8b948f-d2bf-4911-e6f2-0c5e626bf567"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Identified Non-English Words from file2.txt:\n",
            "hagrid: 27\n",
            "ter: 22\n",
            "yeh: 13\n",
            "www: 10\n",
            "ztcprep: 10\n",
            "gringotts: 7\n",
            "didn: 6\n",
            "ernon: 5\n",
            "ll: 5\n",
            "stuf: 3\n",
            "ap: 3\n",
            "knuts: 2\n",
            "izards: 2\n",
            "albus: 2\n",
            "eah: 2\n",
            "ve: 2\n",
            "gettin: 2\n",
            "wasn: 1\n",
            "ou: 1\n",
            "mentionin: 1\n",
            "ophet: 1\n",
            "guardin: 1\n",
            "speakin: 1\n",
            "diagon: 1\n",
            "65: 1\n",
            "payin: 1\n",
            "deliverin: 1\n",
            "70: 1\n",
            "dumbled: 1\n",
            "ying: 1\n",
            "tryin: 1\n",
            "insul: 1\n",
            "meself: 1\n",
            "dif: 1\n",
            "ficult: 1\n",
            "mm: 1\n",
            "everythin: 1\n",
            "goin: 1\n",
            "ther: 1\n",
            "muggles: 1\n",
            "cept: 1\n",
            "72: 1\n",
            "67: 1\n",
            "askin: 1\n",
            "pposed: 1\n",
            "wouldn: 1\n",
            "teh: 1\n",
            "68: 1\n",
            "aren: 1\n",
            "messin: 1\n",
            "fetchin: 1\n",
            "66: 1\n",
            "shouldn: 1\n",
            "69: 1\n",
            "71: 1\n"
          ]
        }
      ]
    }
  ]
}